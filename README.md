ğŸ§  Multimodal Real-Time Vision System

A real-time camera-based AI system that integrates hand gesture recognition, object detection, and emotion recognition using pre-trained deep learning models.
The system is gesture-controlled, enabling or disabling vision modules dynamically for an interactive user experience.

ğŸš€ Features
âœ‹ Hand Gesture Recognition

Detects the following gestures in real time:

Thumbs Up ğŸ‘

Open Palm âœ‹

Fist âœŠ

Peace âœŒï¸

Pointing â˜ï¸

Used as a control signal for activating other modules.

ğŸ“¦ Object Detection

Detects common indoor objects using a YOLO-based model:

Person

Cell Phone

Bottle

Laptop

Chair

Book

Cup

Keyboard

Mouse

Bounding boxes with confidence scores are displayed on screen.

ğŸ™‚ Emotion Recognition

Detects facial emotions (single face per frame):

Happy

Neutral

Sad

Angry

Surprised

Uses a pre-trained CNN for real-time inference.

ğŸ§© Gesture-Controlled Logic

Thumbs Up â†’ Enable object & emotion detection

Fist â†’ Disable all detections

Open Palm â†’ Pause (status only)

This makes the system interactive and intelligent, not just reactive.

ğŸ› ï¸ Tech Stack

Python

OpenCV â€“ camera handling & visualization

MediaPipe â€“ hand gesture recognition

YOLOv8 (Ultralytics) â€“ object detection

TensorFlow / Keras â€“ emotion recognition

NumPy

All models used are pre-trained (no dataset collection or training).

ğŸ“ Project Structure
camera-recognition/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gesture.py
â”‚   â”œâ”€â”€ object.py
â”‚   â”œâ”€â”€ emotion.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ yolo.pt
â”‚   â””â”€â”€ emotion_model.h5
â”‚
â””â”€â”€ assets/
    â””â”€â”€ labels.txt


âš ï¸ Model files are excluded from GitHub using .gitignore.

âš™ï¸ Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/<your-username>/multimodal-vision-system.git
cd multimodal-vision-system

2ï¸âƒ£ Create Virtual Environment
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Add Model Files

Place the following files manually:

models/yolo.pt

models/emotion_model.h5

â–¶ï¸ Run the Project
python main.py


Press q to exit.

ğŸ¯ Output Example
Gesture: Thumbs Up (0.92)
MODE: ACTIVE

ğŸ“¦ Object: Person (0.95)
ğŸ“¦ Object: Laptop (0.88)

ğŸ™‚ Emotion: Happy (0.87)
FPS: 18

ğŸ§  Why This Project Is Advanced

Multimodal AI integration

Real-time inference (FPS â‰¥ 15)

Gesture-controlled system logic

Confidence-based filtering

Clean, explainable architecture

Industry-style use of pre-trained models

ğŸ“ Academic / Interview Explanation

â€œThis project demonstrates a real-time multimodal vision system where hand gestures dynamically control object detection and emotion recognition using pre-trained deep learning models. The focus is on system integration, performance, and reliable inference rather than model training.â€

ğŸ“Œ Future Improvements (Optional)

FPS optimization using threading

Temporal smoothing for predictions

Deployment as a desktop or web app

ğŸ“œ License

This project is for educational and academic use.
